# =============================================================================
# NVIDIA NIM Switch - Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# NVIDIA NIM API Configuration (REQUIRED)
# -----------------------------------------------------------------------------
# Get your API key from: https://build.nvidia.com/explore/discover
NVIDIA_NIM_API_KEY=""

# Default model to use on startup
MODEL="nvidia/llama-3.1-nemotron-70b-instruct"

# -----------------------------------------------------------------------------
# Rate Limiting (Recommended for Production)
# -----------------------------------------------------------------------------
# Number of requests allowed per time window
NVIDIA_NIM_RATE_LIMIT=20

# Time window in seconds
NVIDIA_NIM_RATE_WINDOW=60

# -----------------------------------------------------------------------------
# Model Parameters (Optional - Adjust as needed)
# -----------------------------------------------------------------------------
# Temperature: Controls randomness (0.0 = deterministic, 2.0 = very random)
NVIDIA_NIM_TEMPERATURE=1.0

# Top P: Nucleus sampling threshold (0.0-1.0)
NVIDIA_NIM_TOP_P=1.0

# Top K: Limits vocabulary to top K tokens (-1 = disabled)
NVIDIA_NIM_TOP_K=-1

# Max Tokens: Maximum response length
NVIDIA_NIM_MAX_TOKENS=81920

# Presence Penalty: Penalize tokens that have appeared (-2.0 to 2.0)
NVIDIA_NIM_PRESENCE_PENALTY=0.0

# Frequency Penalty: Penalize frequent tokens (-2.0 to 2.0)
NVIDIA_NIM_FREQUENCY_PENALTY=0.0

# Min P: Minimum probability threshold (0.0-1.0)
NVIDIA_NIM_MIN_P=0.0

# Repetition Penalty: Penalize repeated tokens (1.0 = no penalty)
NVIDIA_NIM_REPETITION_PENALTY=1.0

# Seed: Random seed for reproducibility (empty = random)
NVIDIA_NIM_SEED=

# Stop: Stop sequences (comma-separated, empty = none)
NVIDIA_NIM_STOP=

# -----------------------------------------------------------------------------
# Advanced Model Parameters (Optional)
# -----------------------------------------------------------------------------
# Parallel Tool Calls: Allow multiple tool calls in one response
NVIDIA_NIM_PARALLEL_TOOL_CALLS=true

# Return Tokens as IDs: Return token IDs instead of text
NVIDIA_NIM_RETURN_TOKENS_AS_TOKEN_IDS=false

# Include Stop String: Include stop string in output
NVIDIA_NIM_INCLUDE_STOP_STR_IN_OUTPUT=false

# Ignore EOS: Ignore end-of-sequence token
NVIDIA_NIM_IGNORE_EOS=false

# Min Tokens: Minimum number of tokens to generate
NVIDIA_NIM_MIN_TOKENS=0

# Chat Template: Custom chat template (empty = use model default)
NVIDIA_NIM_CHAT_TEMPLATE=""

# Request ID: Custom request ID for tracking (empty = auto-generated)
NVIDIA_NIM_REQUEST_ID=""

# Reasoning Effort: For reasoning models (low/medium/high)
NVIDIA_NIM_REASONING_EFFORT=high

# Include Reasoning: Show reasoning process in response
NVIDIA_NIM_INCLUDE_REASONING=true

# -----------------------------------------------------------------------------
# Server Configuration (Optional)
# -----------------------------------------------------------------------------
# Host: Server bind address (0.0.0.0 = all interfaces, 127.0.0.1 = localhost only)
HOST=0.0.0.0

# Port: Server port
PORT=8089

# -----------------------------------------------------------------------------
# Notes
# -----------------------------------------------------------------------------
# 1. Only NVIDIA_NIM_API_KEY is required - all other settings have defaults
# 2. You can switch models anytime via web interface or API
# 3. Rate limiting protects your API quota
# 4. For production deployment, see docs/VPS_DEPLOYMENT.md
# 5. For Windows users, see docs/WINDOWS_INSTALLATION.md